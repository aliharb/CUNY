{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "import keras.callbacks as cb\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I'm not using many of these functions, but I'll just keep them here for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)\n",
    "\n",
    "def load_data():\n",
    "    print 'Loading data...'\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "    X_train = np.reshape(X_train, (60000, 784))\n",
    "    X_test = np.reshape(X_test, (10000, 784))\n",
    "\n",
    "    print 'Data loaded.'\n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    start_time = time.time()\n",
    "    print 'Compiling Model ... '\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, input_dim=784))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(300))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rms,\n",
    "      metrics=['accuracy'])\n",
    "    print 'Model compiled in {0} seconds'.format(time.time() - start_time)\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_network(data=None, model=None, epochs=20, batch=256):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        if data is None:\n",
    "            X_train, X_test, y_train, y_test = load_data()\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = data\n",
    "\n",
    "        if model is None:\n",
    "            model = init_model()\n",
    "\n",
    "        history = LossHistory()\n",
    "\n",
    "        print 'Training model...'\n",
    "        model.fit(X_train, y_train, nb_epoch=epochs, batch_size=batch,\n",
    "                  callbacks=[history],\n",
    "                  validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "        print \"Training duration : {0}\".format(time.time() - start_time)\n",
    "        score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "        print \"Network's test score [loss, accuracy]: {0}\".format(score)\n",
    "        return model, history.losses\n",
    "    except KeyboardInterrupt:\n",
    "        print ' KeyboardInterrupt'\n",
    "        return model, history.losses\n",
    "\n",
    "\n",
    "def predict(model, images):\n",
    "  \"\"\"\n",
    "  Takes an array of images. Obviously dimensions must match training set.\n",
    "  \"\"\"\n",
    "  return model.predict_classes(images)\n",
    "\n",
    "\n",
    "def display_classes(png, images, classes, ncol=4):\n",
    "  \"\"\"\n",
    "  Draw a number of images and their predictions\n",
    "  Example:\n",
    "  images = data[1][:12]\n",
    "  classes = model.predict_classes('classes.png', images)\n",
    "  \"\"\"\n",
    "  fig = plt.figure()\n",
    "  nrow = len(images) / ncol\n",
    "  if len(images) % ncol > 0: nrow = nrow + 1\n",
    "\n",
    "  def draw(i):\n",
    "    plt.subplot(nrow,ncol,i)\n",
    "    plt.imshow(images[i].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title('Predicted: %s' % classes[i])\n",
    "  [ draw(i) for i in range(0,len(images)) ]\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(png)\n",
    "\n",
    "def plot_losses(png, losses):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('Loss per batch')\n",
    "    plt.savefig(png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I'll load my data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "#X_train = np.reshape(X_train, (60000, 784))\n",
    "#X_test = np.reshape(X_test, (10000, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-17e8fca52d03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xtrain'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'xtest.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ytrain.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train.to_csv('xtrain.csv')\n",
    "X_test.to_csv('xtest.csv')\n",
    "y_train.to_csv('ytrain.csv')\n",
    "y_test.to_csv('ytest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test, Here is an example of the image viewing function I would use if I needed to check out the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~charleyferrari/961.embed\" height=\"300px\" width=\"300px\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    go.Heatmap(\n",
    "        z=addnoise(np.rot90(X_train[0].transpose()))\n",
    "    )\n",
    "]\n",
    "\n",
    "layout = go.Layout(\n",
    "    height = 300,\n",
    "    width = 300,\n",
    "    autosize = False,\n",
    "    showlegend = False\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='mnist1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm including a function that adds noise to an image. To do this, I'm creating a random matrix, which is a collection of 0's, 1 positve standard deviation, and 1 negative standard deviation. This matrix is basically a random on-off switch, marking which points I'd like to add noise to. Then, I have a second random matrix where I sample from the uniform distribution. This is a random magnitude element. I multiply these two to get my noise, and add the noise to the image matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def addnoise(imagemat):\n",
    "    mean = np.reshape(imagemat, 784).mean()\n",
    "    std = np.reshape(imagemat, 784).std()\n",
    "    randmat = np.random.choice([0,std,-std], size = 784, replace = True, p=[0.75, 0.125, 0.125]).reshape(28, 28)\n",
    "    unimat = np.random.uniform(0,1,784).reshape(28, 28)\n",
    "    return imagemat + randmat*unimat\n",
    "\n",
    "noisyX_train = []\n",
    "for image in X_train:\n",
    "    noisyX_train.append(addnoise(image).reshape(784))\n",
    "noisyX_train = np.array(noisyX_train)\n",
    "#noisyX_train /= 255\n",
    "\n",
    "noisyX_test = []\n",
    "for image in X_test:\n",
    "    noisyX_test.append(addnoise(image).reshape(784))\n",
    "noisyX_test = np.array(noisyX_test)\n",
    "#noisyX_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'm going to create my model. In this case I'm using the same model from the sample code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(500, input_dim=784))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.4))\n",
    "model1.add(Dense(300))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(Dropout(0.4))\n",
    "model1.add(Dense(10))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "rms = RMSprop()\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=rms,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to use bagging in an attempt to improve my model's performance. This bagging function goes through an image group and selects random images for a test set. I could use this function to create as many test sets as I want for an ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "def bagit(images, y_train):\n",
    "    newx = []\n",
    "    newy = []\n",
    "    for i in np.arange(0,60000):\n",
    "        j = np.random.choice(np.arange(0,60000), replace = True)\n",
    "        newx.append(images[j,:])\n",
    "        newy.append(y_train[j,:])\n",
    "    newx = np.array(newx)\n",
    "    newy = np.array(newy)\n",
    "    print 'going'\n",
    "    return (newx,newy)\n",
    "\n",
    "X_1,y_1 = bagit(noisyX_train, y_train)\n",
    "X_2,y_2 = bagit(noisyX_train, y_train)\n",
    "X_3,y_3 = bagit(noisyX_train, y_train)\n",
    "X_4,y_4 = bagit(noisyX_train, y_train)\n",
    "X_5,y_5 = bagit(noisyX_train, y_train)\n",
    "X_6,y_6 = bagit(noisyX_train, y_train)\n",
    "X_7,y_7 = bagit(noisyX_train, y_train)\n",
    "X_8,y_8 = bagit(noisyX_train, y_train)\n",
    "X_9,y_9 = bagit(noisyX_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll use similar code to the above to fit my models. After calculating each fit, I'll use the model to create a y_pred variable that will be added to the ensemble. Right now I only have this including two models (these take a long time to run), but using this method I could easily get 9 y_preds to match my random bagged training sets above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "38s - loss: 0.1115 - acc: 0.9711 - val_loss: 0.1302 - val_acc: 0.9707\n",
      "Epoch 2/20\n",
      "15s - loss: 0.0842 - acc: 0.9768 - val_loss: 0.1250 - val_acc: 0.9712\n",
      "Epoch 3/20\n",
      "15s - loss: 0.0694 - acc: 0.9804 - val_loss: 0.1391 - val_acc: 0.9693\n",
      "Epoch 4/20\n",
      "15s - loss: 0.0604 - acc: 0.9828 - val_loss: 0.1401 - val_acc: 0.9711\n",
      "Epoch 5/20\n",
      "15s - loss: 0.0517 - acc: 0.9849 - val_loss: 0.1321 - val_acc: 0.9734\n",
      "Epoch 6/20\n",
      "15s - loss: 0.0462 - acc: 0.9870 - val_loss: 0.1442 - val_acc: 0.9708\n",
      "Epoch 7/20\n",
      "15s - loss: 0.0440 - acc: 0.9876 - val_loss: 0.1451 - val_acc: 0.9723\n",
      "Epoch 8/20\n",
      "15s - loss: 0.0382 - acc: 0.9890 - val_loss: 0.1431 - val_acc: 0.9734\n",
      "Epoch 9/20\n",
      "15s - loss: 0.0367 - acc: 0.9897 - val_loss: 0.1445 - val_acc: 0.9735\n",
      "Epoch 10/20\n",
      "15s - loss: 0.0323 - acc: 0.9909 - val_loss: 0.1589 - val_acc: 0.9734\n",
      "Epoch 11/20\n",
      "15s - loss: 0.0332 - acc: 0.9912 - val_loss: 0.1585 - val_acc: 0.9735\n",
      "Epoch 12/20\n",
      "16s - loss: 0.0317 - acc: 0.9913 - val_loss: 0.1622 - val_acc: 0.9738\n",
      "Epoch 13/20\n",
      "15s - loss: 0.0298 - acc: 0.9922 - val_loss: 0.1572 - val_acc: 0.9740\n",
      "Epoch 14/20\n",
      "15s - loss: 0.0272 - acc: 0.9929 - val_loss: 0.1625 - val_acc: 0.9744\n",
      "Epoch 15/20\n",
      "15s - loss: 0.0270 - acc: 0.9930 - val_loss: 0.1664 - val_acc: 0.9744\n",
      "Epoch 16/20\n",
      "15s - loss: 0.0261 - acc: 0.9930 - val_loss: 0.1804 - val_acc: 0.9721\n",
      "Epoch 17/20\n",
      "16s - loss: 0.0252 - acc: 0.9931 - val_loss: 0.1833 - val_acc: 0.9739\n",
      "Epoch 18/20\n",
      "15s - loss: 0.0244 - acc: 0.9938 - val_loss: 0.1825 - val_acc: 0.9738\n",
      "Epoch 19/20\n",
      "15s - loss: 0.0230 - acc: 0.9939 - val_loss: 0.1766 - val_acc: 0.9737\n",
      "Epoch 20/20\n",
      "15s - loss: 0.0225 - acc: 0.9940 - val_loss: 0.1907 - val_acc: 0.9724\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "35s - loss: 0.0708 - acc: 0.9846 - val_loss: 0.1578 - val_acc: 0.9733\n",
      "Epoch 2/20\n",
      "15s - loss: 0.0578 - acc: 0.9872 - val_loss: 0.1453 - val_acc: 0.9740\n",
      "Epoch 3/20\n",
      "15s - loss: 0.0455 - acc: 0.9885 - val_loss: 0.1561 - val_acc: 0.9759\n",
      "Epoch 4/20\n",
      "15s - loss: 0.0395 - acc: 0.9901 - val_loss: 0.1559 - val_acc: 0.9759\n",
      "Epoch 5/20\n",
      "15s - loss: 0.0363 - acc: 0.9912 - val_loss: 0.1735 - val_acc: 0.9737\n",
      "Epoch 6/20\n",
      "15s - loss: 0.0334 - acc: 0.9919 - val_loss: 0.1650 - val_acc: 0.9743\n",
      "Epoch 7/20\n",
      "15s - loss: 0.0337 - acc: 0.9922 - val_loss: 0.1664 - val_acc: 0.9760\n",
      "Epoch 8/20\n",
      "15s - loss: 0.0267 - acc: 0.9931 - val_loss: 0.1731 - val_acc: 0.9756\n",
      "Epoch 9/20\n",
      "15s - loss: 0.0258 - acc: 0.9939 - val_loss: 0.1718 - val_acc: 0.9750\n",
      "Epoch 10/20\n",
      "15s - loss: 0.0263 - acc: 0.9939 - val_loss: 0.1740 - val_acc: 0.9756\n",
      "Epoch 11/20\n",
      "16s - loss: 0.0243 - acc: 0.9942 - val_loss: 0.1661 - val_acc: 0.9767\n",
      "Epoch 12/20\n",
      "15s - loss: 0.0237 - acc: 0.9942 - val_loss: 0.1818 - val_acc: 0.9749\n",
      "Epoch 13/20\n",
      "15s - loss: 0.0246 - acc: 0.9941 - val_loss: 0.1858 - val_acc: 0.9735\n",
      "Epoch 14/20\n",
      "15s - loss: 0.0212 - acc: 0.9951 - val_loss: 0.1843 - val_acc: 0.9749\n",
      "Epoch 15/20\n",
      "15s - loss: 0.0230 - acc: 0.9949 - val_loss: 0.1882 - val_acc: 0.9736\n",
      "Epoch 16/20\n",
      "15s - loss: 0.0211 - acc: 0.9952 - val_loss: 0.1888 - val_acc: 0.9756\n",
      "Epoch 17/20\n",
      "16s - loss: 0.0209 - acc: 0.9948 - val_loss: 0.1887 - val_acc: 0.9737\n",
      "Epoch 18/20\n",
      "15s - loss: 0.0189 - acc: 0.9955 - val_loss: 0.1759 - val_acc: 0.9764\n",
      "Epoch 19/20\n",
      "15s - loss: 0.0202 - acc: 0.9954 - val_loss: 0.1899 - val_acc: 0.9750\n",
      "Epoch 20/20\n",
      "17s - loss: 0.0180 - acc: 0.9959 - val_loss: 0.1920 - val_acc: 0.9749\n"
     ]
    }
   ],
   "source": [
    "history = LossHistory()\n",
    "epochs=20\n",
    "batch=256\n",
    "\n",
    "\n",
    "model1.fit(X_1, y_1, nb_epoch=epochs, batch_size=batch,\n",
    "          callbacks=[history],\n",
    "          validation_data=(noisyX_test, y_test), verbose=2)\n",
    "y_pred1 = model1.predict(noisyX_test, verbose = False)\n",
    "\n",
    "\n",
    "model1.fit(X_2, y_2, nb_epoch=epochs, batch_size=batch,\n",
    "          callbacks=[history],\n",
    "          validation_data=(noisyX_test, y_test), verbose=2)\n",
    "y_pred2 = model1.predict(noisyX_test, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm defining a helper function called \"arraytonumber\". The y_test data is in the form of 0 or 1 matrices, with a single '1' in the position corresponding to what the digit is. The result of my models are in a similar form, only these are probabilities. This will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arraytonumber(arr):\n",
    "    check = 0\n",
    "    index = 0\n",
    "    for i in np.arange(0,10):\n",
    "        if arr[i] > check:\n",
    "            index = i\n",
    "            check = arr[i]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I could have my ensembles \"vote\" on what the prediction would be. To do this, I simply take the mean of each of the probabilities to come up with a single probability prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.array([y_pred1, y_pred2]).mean(axis = 0)\n",
    "\n",
    "y_pred_c = np.apply_along_axis(arraytonumber, 1, y_pred)\n",
    "\n",
    "y_test_c = np.apply_along_axis(arraytonumber, 1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, lets see how I did. I'm using an ensemble strategy, so I can't use Keras' built in functions to check my models' health. I'll have to manually calculate my accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp = (y_pred_c == y_test_c)\n",
    "vint = np.vectorize(int)\n",
    "vcomp = vint(comp)\n",
    "\n",
    "float(vcomp.sum()) / float(len(vcomp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far with two models I ended up at 97.5%. This is still not beating what Prof Rowe came up with using a single neural net without noise, so I'll have to keep on trying to beef up my accuracy..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
