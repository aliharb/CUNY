{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 400                                   # training sample size\n",
    "feats = 784                               # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in product(np.arange(0,5), np.arange(0,5)):\n",
    "    data.append([i[0], i[1], 2*(i[0]**2) - 3*(i[1]**2) + 1])\n",
    "    \n",
    "data = np.array(data)\n",
    "\n",
    "#x = data[:,[0,1]]\n",
    "#y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "w = theano.shared(rng.randn(2), name=\"w\")\n",
    "\n",
    "# initialize the bias term\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "print(\"Initial model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_steps = 10000\n",
    "\n",
    "prediction = T.dot(x, w) + b\n",
    "costIndv = (prediction - y)**2 \n",
    "cost = T.sum(costIndv)\n",
    "gw, gb = T.grad(cost, [w,b])\n",
    "\n",
    "train = theano.function(\n",
    "    inputs = [x,y],\n",
    "    outputs = [prediction, costIndv],\n",
    "    updates = ((w, w-0.1*gw), (b, b-0.1*gb))\n",
    ")\n",
    "predict = theano.function(inputs = [x], outputs = prediction)\n",
    "\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(data[:,[0,1]], data[:,2])\n",
    "    if i % 500 == 0: #only print the cost every 500 epochs/iterations (to save space)\n",
    "        print('Cost: %s' % (err,))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model:\n",
      "[ 0.48972425 -0.3355455 ]\n",
      "0.0\n",
      "Final model:\n",
      "[ 2. -3.]\n",
      "1.0\n",
      "target values for D:\n",
      "[  1  -2 -11 -26 -47   3   0  -9 -24 -45   9   6  -3 -18 -39  19  16   7\n",
      "  -8 -29  33  30  21   6 -15]\n",
      "prediction on D:\n",
      "[  1.00000000e+00  -2.00000000e+00  -1.10000000e+01  -2.60000000e+01\n",
      "  -4.70000000e+01   3.00000000e+00  -4.18554080e-14  -9.00000000e+00\n",
      "  -2.40000000e+01  -4.50000000e+01   9.00000000e+00   6.00000000e+00\n",
      "  -3.00000000e+00  -1.80000000e+01  -3.90000000e+01   1.90000000e+01\n",
      "   1.60000000e+01   7.00000000e+00  -8.00000000e+00  -2.90000000e+01\n",
      "   3.30000000e+01   3.00000000e+01   2.10000000e+01   6.00000000e+00\n",
      "  -1.50000000e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "\n",
    "#N = 400                                   # training sample size\n",
    "#feats = 784                               # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "#D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "w = theano.shared(rng.randn(2), name=\"w\")\n",
    "\n",
    "# initialize the bias term\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "print(\"Initial model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "\n",
    "# Construct Theano expression graph\n",
    "#p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
    "#prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "#xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
    "#cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
    "#gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
    "                                          # w.r.t weight vector w and\n",
    "                                          # bias term b\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "prediction = T.dot((x**2),w) + b\n",
    "xent = (prediction - y)**2\n",
    "cost = xent.sum()\n",
    "gw, gb = T.grad(cost, [w,b])\n",
    "                \n",
    "                # Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.0001 * gw), (b, b - 0.0001 * gb)))\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(100000):\n",
    "    pred, err = train(data[:,[0,1]], data[:,2])\n",
    "    \n",
    "#x = data[:,[0,1]]\n",
    "#y = data[:,2]\n",
    "\n",
    "print(\"Final model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(data[:,2])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(data[:,[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model:\n",
      "[-0.70401986  0.68193498  0.58770806]\n",
      "Final model:\n",
      "[ 1.  2. -3.]\n",
      "0.0\n",
      "target values for D:\n",
      "[  1  -2 -11 -26 -47   3   0  -9 -24 -45   9   6  -3 -18 -39  19  16   7\n",
      "  -8 -29  33  30  21   6 -15]\n",
      "prediction on D:\n",
      "[  1.00000000e+00  -2.00000000e+00  -1.10000000e+01  -2.60000000e+01\n",
      "  -4.70000000e+01   3.00000000e+00  -3.81916720e-14  -9.00000000e+00\n",
      "  -2.40000000e+01  -4.50000000e+01   9.00000000e+00   6.00000000e+00\n",
      "  -3.00000000e+00  -1.80000000e+01  -3.90000000e+01   1.90000000e+01\n",
      "   1.60000000e+01   7.00000000e+00  -8.00000000e+00  -2.90000000e+01\n",
      "   3.30000000e+01   3.00000000e+01   2.10000000e+01   6.00000000e+00\n",
      "  -1.50000000e+01]\n"
     ]
    }
   ],
   "source": [
    "def layer(x, w):\n",
    "    b = np.array(np.repeat(1,data[:,[0,1]].shape[0]).reshape(-1,1), dtype = theano.config.floatX)\n",
    "    new_x = T.concatenate([b, x], axis=1)\n",
    "    m = T.dot(w, new_x.T).T #theta1: 3x3 * x: 3x1 = 3x1 ;;; theta2: 1x4 * 4x1\n",
    "    h = nnet.relu(m)\n",
    "    return h\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "rng = numpy.random\n",
    "\n",
    "#N = 400                                   # training sample size\n",
    "#feats = 784                               # number of input variables\n",
    "\n",
    "# generate a dataset: D = (input_values, target_class)\n",
    "#D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "training_steps = 10000\n",
    "\n",
    "# Declare Theano symbolic variables\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "# initialize the weight vector w randomly\n",
    "#\n",
    "# this and the following bias variable b\n",
    "# are shared so they keep their values\n",
    "# between training iterations (updates)\n",
    "theta1 = theano.shared(np.array(np.random.rand(3,3), dtype=theano.config.floatX), name = 'theta1')\n",
    "theta2 = theano.shared(np.array(np.random.rand(4,1), dtype=theano.config.floatX), name = 'theta2')\n",
    "\n",
    "# initialize the bias term\n",
    "\n",
    "print(\"Initial model:\")\n",
    "print(theta1.get_value())\n",
    "print(theta2.get_value())\n",
    "#print(b.get_value())\n",
    "\n",
    "# Construct Theano expression graph\n",
    "#p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))   # Probability that target = 1\n",
    "#prediction = p_1 > 0.5                    # The prediction thresholded\n",
    "#xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) # Cross-entropy loss function\n",
    "#cost = xent.mean() + 0.01 * (w ** 2).sum()# The cost to minimize\n",
    "#gw, gb = T.grad(cost, [w, b])             # Compute the gradient of the cost\n",
    "                                          # w.r.t weight vector w and\n",
    "                                          # bias term b\n",
    "                                          # (we shall return to this in a\n",
    "                                          # following section of this tutorial)\n",
    "\n",
    "#np.concatenate([np.repeat(1,data[:,[0,1]].shape[0]).reshape(-1,1), data[:,[0,1]]], axis=1)\n",
    "new_x = T.concatenate([np.repeat(1,data[:,[0,1]].shape[0]).reshape(-1,1), x], axis=1)\n",
    "prediction = T.dot((new_x**2),w)\n",
    "xent = (prediction - y)**2\n",
    "cost = xent.sum()\n",
    "gw= T.grad(cost, w)\n",
    "\n",
    "h = layer(x, theta1)\n",
    "prediction = layer(x, theta2)\n",
    "xent = (prediction - y)**2\n",
    "cost = xent.sum()\n",
    "gw = T\n",
    "                \n",
    "                # Compile\n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=[(w, w - 0.0001 * gw)])\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "\n",
    "# Train\n",
    "for i in range(100000):\n",
    "    pred, err = train(data[:,[0,1]], data[:,2])\n",
    "    \n",
    "#x = data[:,[0,1]]\n",
    "#y = data[:,2]\n",
    "\n",
    "print(\"Final model:\")\n",
    "print(w.get_value())\n",
    "print(b.get_value())\n",
    "print(\"target values for D:\")\n",
    "print(data[:,2])\n",
    "print(\"prediction on D:\")\n",
    "print(predict(data[:,[0,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.array([a,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([2, 3]), 4], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 2],\n",
       "       [1, 0, 3],\n",
       "       [1, 0, 4],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 2],\n",
       "       [1, 1, 3],\n",
       "       [1, 1, 4],\n",
       "       [1, 2, 0],\n",
       "       [1, 2, 1],\n",
       "       [1, 2, 2],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 4],\n",
       "       [1, 3, 0],\n",
       "       [1, 3, 1],\n",
       "       [1, 3, 2],\n",
       "       [1, 3, 3],\n",
       "       [1, 3, 4],\n",
       "       [1, 4, 0],\n",
       "       [1, 4, 1],\n",
       "       [1, 4, 2],\n",
       "       [1, 4, 3],\n",
       "       [1, 4, 4]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.repeat(1,data[:,[0,1]].shape[0]).reshape(-1,1), data[:,[0,1]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(1,data[:,[0,1]].shape[0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(\"The 'tensors' argument must be either a tuple or a list, make sure you did not forget () or [] around arguments of concatenate.\", x)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-8cc516ae8d6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/theano/tensor/basic.pyc\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(tensor_list, axis)\u001b[0m\n\u001b[0;32m   4222\u001b[0m             \u001b[1;34m\"The 'tensors' argument must be either a tuple \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4223\u001b[0m             \u001b[1;34m\"or a list, make sure you did not forget () or [] around \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4224\u001b[1;33m             \"arguments of concatenate.\", tensor_list)\n\u001b[0m\u001b[0;32m   4225\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: (\"The 'tensors' argument must be either a tuple or a list, make sure you did not forget () or [] around arguments of concatenate.\", x)"
     ]
    }
   ],
   "source": [
    "new_x = T.concatenate(x, np.repeat(1,data[:,[0,1]].shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
