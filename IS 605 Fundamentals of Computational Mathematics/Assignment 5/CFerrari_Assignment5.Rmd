---
title: "CFerrari_Assignment5"
author: "Charley Ferrari"
date: "Friday, February 27, 2015"
output: pdf_document
---


Problem Set 1
============

In this problem set we'll work out some properties of the least squares solution that we reviewed in the weekly readings. Consider the unsolvable system $Ax = b$ as given below:

$$
\left[ {\begin{array}{cc}
1 & 0\\
1 & 1\\
1 & 3\\
1 & 4\\
\end{array} } \right]
\left[ {\begin{array}{c}
x_1\\
x_2\\
\end{array} } \right]
=
\left[ {\begin{array}{c}
0\\
8\\
8\\
19\\
\end{array} } \right]$$

--Write R markdown script to compute $A^T A$ and $A^T b$

```{r}

A <- matrix(c(1,1,1,1,0,1,3,4),nrow=4)
b <- matrix(c(0,8,8,20), nrow=4)

AtA <- t(A) %*% A
Atb <- t(A) %*% b

AtA

Atb


```

--Solve for $x$ in $R$ using the above two computed matrices.

```{r}


xhat <- solve(AtA) %*% Atb

xhat

```

--What is the squared error of this solution?

$A \hat x - b = e$

$||e||^2 = ||A \hat x - b||^2$

```{r}

esquared <- t((A %*% xhat) - b) %*% ((A %*% xhat) - b)

esquared

```

--Instead of b = [0 8 8 20], start with p = [1 5 13 17] and find the exact solution
(i.e. show that this system is solvable as all equations are consistent with each
other. This should result in an error vector e = 0).

$Ax = p$

```{r}

p <- matrix(c(1,5,13,17),nrow=4)

library(pracma)

rref(cbind(A,p))

```

As you can see in the RREF form of the matrix, we get $x_1 = 1$ and $x_2 = 4$
looking at the original problem as a system of equations:

$x_1 = 1$

$x_1 + x_2 = 5$ This already gives the solution set $x_1 = 1$ and $x_2 = 4$

$x_1 + 3x_2 = 13$ This is correct

$x_1 + 4x_2 = 17$ This is also correct

-- Show that the error $e = b - p$ = [-1 3 -5 2].

```{r}

e <- b - p

e

```

-- Show that the error $e$ is orthogonal to $p$ and to each of the columns of $A$.

Lets compute the dot products

```{r}

t(e) %*% p

t(e) %*% A[,1]

t(e) %*% A[,2]

```


Problem Set 2
============


Consider the modified auto-mpg data (obtained from the UC Irvine Machine Learning
dataset). This dataset contains 5 columns: displacement, horsepower, weight, acceleration,
mpg. We are going to model mpg as a function of the other four variables.
Write an R markdown script that takes in the auto-mpg data, extracts an A matrix
from the first 4 columns and b vector from the fifth (mpg) column. Using the least squares
approach, your code should compute the best fitting solution. That is, find the best fitting
equation that expresses mpg in terms of the other 4 variables. Finally, calculate the fitting
error between the predicted error of your model and the actual mpg. Your script should
be able to load in the 5 column data set, extract A and b, and perform the rest of the
calculations. Please have adequate comments in your code to make it easy to follow your
work

First, I'll read in the data.
It seems to be in a weird format, so I used scan, and coerced it into a matrix:

```{r}

setwd("E:/Downloads/Courses/CUNY/SPS/Git/IS 605 Fundamentals of Computational Mathematics/Assignment 5")

autodata <- scan("auto-mpg.data")
autodata <- t(matrix(autodata, nrow = 5))

```

Next I'll create my A matrix and b vector:

```{r}

Adata <- autodata[,1:4]
bdata <- autodata[,5]

```

Now to find my $A^t A$ and $A^t b$:

```{r}
AtAdata <- t(Adata) %*% Adata
Atbdata <- t(Adata) %*% bdata

AtAdata
Atbdata

```



And to solve for $\hat x$:

```{r}

xhatdata <- solve(AtAdata) %*% Atbdata

xhatdata

```

This $\hat x$ is a vector of the coefficients to my equation $Ax = b$ that minimizes
the square errors.

This equation can be written as:

$$-0.030037938 \times displacement + 0.157115685 \times horsepower -$$
$$0.006217883 \times weight + 1.997320955 \times acceleration = mpg$$

Interestingly, this didn't match what the check I ran using the lm function, and I realized
the intercept was missing from this model. Once I coerced a 0 intercept, I was able to get
everything to match:

```{r}

lm(autodata[,5] ~ 0 + autodata[,1:4])

```

Looking into an EViews econometric textbook (having never really understood their linear
algebra interpretation of regression), I realized all I needed to do was add a "1" column
to my A matrix:

```{r}
AdataInt <- cbind(1,autodata[,1:4])
bdataInt <- autodata[,5]

AtAdataInt <- t(AdataInt) %*% AdataInt
AtbdataInt <- t(AdataInt) %*% bdataInt

xhatdataInt <- solve(AtAdataInt) %*% AtbdataInt

xhatdataInt

lm(autodata[,5] ~ autodata[,1:4])

```

